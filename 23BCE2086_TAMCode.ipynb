{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64ee939",
   "metadata": {},
   "source": [
    "# EDA: Crimes Against Women in India - TAM Round 1 23BCE2086\n",
    "\n",
    "## Overview\n",
    "I had created this notebook which performs comprehensive EDA on crimes against women data, including identification of high-crime states, clustering analysis, and crime type distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf80b17",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e50d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: 'c:\\\\Users\\\\SHRIDHARAN VK\\\\Desktop\\\\TAM RECRUITMENT\\\\Technical/CrimesOnWomenData.csv'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\SHRIDHARAN VK\\\\Desktop\\\\TAM RECRUITMENT\\\\Technical/CrimesOnWomenData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2443956979.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CrimesOnWomenData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'description.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data loaded successfully from {file_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\SHRIDHARAN VK\\\\Desktop\\\\TAM RECRUITMENT\\\\Technical/CrimesOnWomenData.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# make the plots look clean and professional\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# load the data from GitHub\n",
    "github_url = \"https://raw.githubusercontent.com/VK-SHRIDHARAN/23BCE2086-EDA-On-Women-Safety_TAM/main/\"\n",
    "\n",
    "try:\n",
    "    # load from GitHub\n",
    "    df = pd.read_csv(github_url + 'CrimesOnWomenData.csv', index_col=0)\n",
    "    description = pd.read_csv(github_url + 'description.csv', index_col=0)\n",
    "    print(f\"Data loaded successfully from GitHub\")\n",
    "except:\n",
    "    # fallback to local if available\n",
    "    try:\n",
    "        df = pd.read_csv('CrimesOnWomenData.csv', index_col=0)\n",
    "        description = pd.read_csv('description.csv', index_col=0)\n",
    "        print(\"Data loaded from local directory\")\n",
    "    except:\n",
    "        print(\"Error: Could not load data from GitHub or local directory\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a88e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any issues with the data\n",
    "print(\"\\nChecking data quality...\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Date range: {df['Year'].min()} to {df['Year'].max()}\")\n",
    "print(f\"Total states: {df['State'].nunique()}\")\n",
    "\n",
    "# fill any missing values with 0\n",
    "df = df.fillna(0)\n",
    "print(\"\\nData is ready for analysis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b45f7",
   "metadata": {},
   "source": [
    "## 2. Task 1: Identify States with Highest Crime Against Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the crime types and their full names\n",
    "crimes = ['Rape', 'K&A', 'DD', 'AoW', 'AoM', 'DV', 'WT']\n",
    "crime_labels = {\n",
    "    'Rape': 'Rape',\n",
    "    'K&A': 'Kidnapping & Assault',\n",
    "    'DD': 'Dowry Deaths',\n",
    "    'AoW': 'Assault on Women',\n",
    "    'AoM': 'Assault on Modesty',\n",
    "    'DV': 'Domestic Violence',\n",
    "    'WT': 'Women Trafficking'\n",
    "}\n",
    "\n",
    "# calculate total crimes by state\n",
    "state_totals = df.groupby('State')[crimes].sum().sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 States - Total Cases:\")\n",
    "for i, (state, count) in enumerate(state_totals.head(15).items(), 1):\n",
    "    print(f\"{i:2}. {state:25} {count:>10,.0f}\")\n",
    "\n",
    "# also show average per year\n",
    "state_avg = df.groupby('State')[crimes].sum().sum(axis=1) / df.groupby('State').size()\n",
    "state_avg = state_avg.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n\\nTop 15 States - Average per Year:\")\n",
    "for i, (state, count) in enumerate(state_avg.head(15).items(), 1):\n",
    "    print(f\"{i:2}. {state:25} {count:>10,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiple visualizations to show top states from different angles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# bar chart - total cases\n",
    "state_totals.head(15).plot(kind='bar', ax=axes[0, 0], color='darkred', alpha=0.8)\n",
    "axes[0, 0].set_title('Top 15 States - Total Cases', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Total Cases')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# bar chart - average per year\n",
    "state_avg.head(15).plot(kind='bar', ax=axes[0, 1], color='crimson', alpha=0.8)\n",
    "axes[0, 1].set_title('Top 15 States - Average per Year', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Avg Cases per Year')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# pie chart - top 10 states share\n",
    "top_10 = state_totals.head(10)\n",
    "others_sum = state_totals.iloc[10:].sum()\n",
    "pie_data = pd.concat([top_10, pd.Series({'Others': others_sum})])\n",
    "\n",
    "colors_pie = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))\n",
    "axes[1, 0].pie(pie_data, labels=pie_data.index, autopct='%1.1f%%', startangle=90, colors=colors_pie)\n",
    "axes[1, 0].set_title('Crime Share - Top 10 States', fontsize=13, fontweight='bold')\n",
    "\n",
    "# donut chart - top 5 states\n",
    "top_5 = state_totals.head(5)\n",
    "rest_total = state_totals.iloc[5:].sum()\n",
    "donut_data = pd.concat([top_5, pd.Series({'Other States': rest_total})])\n",
    "\n",
    "wedges, texts, autotexts = axes[1, 1].pie(donut_data, labels=donut_data.index, autopct='%1.1f%%', \n",
    "                                           startangle=90, colors=plt.cm.Pastel1(np.linspace(0, 1, len(donut_data))))\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "axes[1, 1].add_artist(centre_circle)\n",
    "axes[1, 1].set_title('Crime Share - Top 5 States (Donut)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_top_crime_states.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n[SAVED] 01_top_crime_states.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b2405",
   "metadata": {},
   "source": [
    "## Visualizations - Top Crime States\n",
    "\n",
    "Four different views of which states have the most crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for clustering\n",
    "state_data = df.groupby('State')[crimes].sum()\n",
    "\n",
    "# normalize the data so all crimes are on the same scale\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(state_data)\n",
    "\n",
    "# test different numbers of clusters to find the best one\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(scaled, km.labels_))\n",
    "\n",
    "# create a 2x2 grid showing different ways to evaluate cluster quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# elbow curve - inertia line plot\n",
    "axes[0, 0].plot(k_range, inertias, 'bo-', linewidth=2.5, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters')\n",
    "axes[0, 0].set_ylabel('Inertia')\n",
    "axes[0, 0].set_title('Elbow Method - Inertia Curve', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# silhouette scores line plot\n",
    "axes[0, 1].plot(k_range, silhouettes, 'go-', linewidth=2.5, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Clusters')\n",
    "axes[0, 1].set_ylabel('Silhouette Score')\n",
    "axes[0, 1].set_title('Silhouette Analysis', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# silhouette scores as bars\n",
    "axes[1, 0].bar(k_range, silhouettes, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Number of Clusters')\n",
    "axes[1, 0].set_ylabel('Silhouette Score')\n",
    "axes[1, 0].set_title('Cluster Quality Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# inertia as bars\n",
    "axes[1, 1].bar(k_range, inertias, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Number of Clusters')\n",
    "axes[1, 1].set_ylabel('Inertia')\n",
    "axes[1, 1].set_title('Inertia Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('02_elbow_silhouette.png', dpi=300, bbox_inches='tight')\n",
    "print(\"[SAVED] 02_elbow_silhouette.png\")\n",
    "plt.show()\n",
    "\n",
    "# use 4 clusters (this is a good balance based on the analysis)\n",
    "optimal_k = 4\n",
    "km = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = km.fit_predict(scaled)\n",
    "state_data['Cluster'] = clusters\n",
    "\n",
    "print(f\"\\nOptimal clusters found: {optimal_k}\")\n",
    "print(\"\\nBreakdown by cluster:\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_states = state_data[state_data['Cluster'] == i].index.tolist()\n",
    "    total_crimes = state_data[state_data['Cluster'] == i][crimes].sum().sum()\n",
    "    print(f\"\\nCluster {i}: {len(cluster_states)} states, {int(total_crimes):,} total crimes\")\n",
    "    if len(cluster_states) <= 5:\n",
    "        print(f\"  {', '.join(cluster_states)}\")\n",
    "    else:\n",
    "        print(f\"  {', '.join(cluster_states[:5])}... and {len(cluster_states)-5} more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be641540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters using PCA (reduce to 2 dimensions for plotting)\n",
    "pca = PCA(n_components=2)\n",
    "pca_coords = pca.fit_transform(scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# scatter plot with state labels\n",
    "scatter = axes[0].scatter(pca_coords[:, 0], pca_coords[:, 1], c=clusters, cmap='viridis', \n",
    "                         s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "for idx, state in enumerate(state_data.index):\n",
    "    axes[0].annotate(state, (pca_coords[idx, 0], pca_coords[idx, 1]), \n",
    "                    fontsize=8, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('State Clusters - PCA Visualization', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# pie chart showing cluster sizes\n",
    "cluster_counts = [sum(clusters == i) for i in range(optimal_k)]\n",
    "cluster_names = [f'Cluster {i}\\n({cluster_counts[i]} states)' for i in range(optimal_k)]\n",
    "colors_clusters = plt.cm.viridis(np.linspace(0, 1, optimal_k))\n",
    "\n",
    "axes[1].pie(cluster_counts, labels=cluster_names, autopct='%1.1f%%', startangle=90, colors=colors_clusters)\n",
    "axes[1].set_title('Distribution of States Across Clusters', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_state_clusters.png', dpi=300, bbox_inches='tight')\n",
    "print(\"[SAVED] 03_state_clusters.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e28d0",
   "metadata": {},
   "source": [
    "## 4. Task 3: Determine States with Highest Cases by Crime Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze crime types by state\n",
    "crime_by_state = df.groupby('State')[crimes].sum()\n",
    "\n",
    "print(\"Which state has the most cases for each crime type:\")\n",
    "for crime in crimes:\n",
    "    top_state = crime_by_state[crime].idxmax()\n",
    "    count = crime_by_state[crime].max()\n",
    "    print(f\"  {crime_labels.get(crime, crime):25} - {top_state:25} ({int(count):,})\")\n",
    "\n",
    "# create a 2x2 grid of crime analysis visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# heatmap of top 15 states and all crime types\n",
    "top_15_states = state_data[crimes].sum(axis=1).nlargest(15).index\n",
    "hmap_data = crime_by_state.loc[top_15_states, crimes]\n",
    "\n",
    "sns.heatmap(hmap_data, annot=True, fmt='d', cmap='YlOrRd', \n",
    "           cbar_kws={'label': 'Cases'}, ax=axes[0, 0], linewidths=0.5)\n",
    "axes[0, 0].set_title('Crime Distribution Heatmap - Top 15 States', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Crime Type')\n",
    "axes[0, 0].set_ylabel('State')\n",
    "\n",
    "# pie chart - overall crime type distribution\n",
    "crime_totals = df[crimes].sum()\n",
    "colors_pie = plt.cm.Set3(np.linspace(0, 1, len(crimes)))\n",
    "axes[0, 1].pie(crime_totals, labels=[crime_labels[c] for c in crimes], autopct='%1.1f%%', \n",
    "              startangle=90, colors=colors_pie)\n",
    "axes[0, 1].set_title('Overall Crime Type Distribution', fontsize=13, fontweight='bold')\n",
    "\n",
    "# horizontal bar chart - total by crime type\n",
    "crime_totals.sort_values(ascending=True).plot(kind='barh', ax=axes[1, 0], \n",
    "                                              color=plt.cm.RdYlGn_r(np.linspace(0, 1, len(crimes))),\n",
    "                                              edgecolor='black')\n",
    "axes[1, 0].set_title('Total Cases by Crime Type', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Total Cases')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# percentage distribution\n",
    "crime_pct = (crime_totals / crime_totals.sum() * 100).sort_values(ascending=True)\n",
    "axes[1, 1].barh(range(len(crime_pct)), crime_pct.values, color=plt.cm.Spectral(np.linspace(0, 1, len(crimes))),\n",
    "               edgecolor='black')\n",
    "axes[1, 1].set_yticks(range(len(crime_pct)))\n",
    "axes[1, 1].set_yticklabels([crime_labels[c] for c in crime_pct.index])\n",
    "axes[1, 1].set_title('Crime Type Percentage Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Percentage (%)')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_crime_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n[SAVED] 04_crime_analysis.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top states for each crime type individually\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, crime in enumerate(crimes):\n",
    "    top_10 = crime_by_state[crime].nlargest(10)\n",
    "    top_10.plot(kind='barh', ax=axes[idx], color=plt.cm.Set3(idx), alpha=0.8, edgecolor='black')\n",
    "    axes[idx].set_title(f'Top 10: {crime_labels[crime]}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Cases')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# remove the last empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('05_crime_types_detail.png', dpi=300, bbox_inches='tight')\n",
    "print(\"[SAVED] 05_crime_types_detail.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84f71b",
   "metadata": {},
   "source": [
    "## 5. Crime Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab115a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze how crimes changed from year to year\n",
    "by_year = df.groupby('Year')[crimes].sum()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# line chart - total crimes over time\n",
    "total_by_year = by_year.sum(axis=1)\n",
    "axes[0, 0].plot(by_year.index, total_by_year, marker='o', linewidth=2.5, \n",
    "               markersize=8, color='darkred')\n",
    "axes[0, 0].fill_between(by_year.index, total_by_year, alpha=0.3, color='red')\n",
    "axes[0, 0].set_title('Total Crimes Over Time', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Cases')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# line chart - each crime type trend\n",
    "for crime in crimes:\n",
    "    axes[0, 1].plot(by_year.index, by_year[crime], marker='o', \n",
    "                   label=crime_labels.get(crime, crime), linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Trends by Crime Type', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Year')\n",
    "axes[0, 1].set_ylabel('Cases')\n",
    "axes[0, 1].legend(loc='best', fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# bar chart - year-over-year growth rate\n",
    "growth = total_by_year.pct_change() * 100\n",
    "axes[1, 0].bar(growth.index[1:], growth.values[1:], color='steelblue', alpha=0.8, edgecolor='black')\n",
    "axes[1, 0].set_title('Year-over-Year Growth Rate', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Growth Rate (%)')\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# stacked area - crime types composition over time\n",
    "ax_stack = axes[1, 1]\n",
    "ax_stack.stackplot(by_year.index, [by_year[crime] for crime in crimes], \n",
    "                   labels=[crime_labels[crime] for crime in crimes], alpha=0.8)\n",
    "ax_stack.set_title('Crime Type Composition Over Time', fontsize=13, fontweight='bold')\n",
    "ax_stack.set_xlabel('Year')\n",
    "ax_stack.set_ylabel('Cases')\n",
    "ax_stack.legend(loc='upper left', fontsize=9)\n",
    "ax_stack.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('06_crime_trends.png', dpi=300, bbox_inches='tight')\n",
    "print(\"[SAVED] 06_crime_trends.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb28978",
   "metadata": {},
   "source": [
    "## 6. Key Insights & Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727dd7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the key findings from the analysis\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS FROM THE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_crimes = df[crimes].sum().sum()\n",
    "print(f\"\\nTotal crimes analyzed: {int(total_crimes):,} cases\")\n",
    "\n",
    "print(\"\\nCrime type breakdown:\")\n",
    "crime_totals = df[crimes].sum().sort_values(ascending=False)\n",
    "for crime, count in crime_totals.items():\n",
    "    pct = (count / total_crimes) * 100\n",
    "    print(f\"  {crime_labels.get(crime, crime):25} {pct:5.1f}% ({int(count):,})\")\n",
    "\n",
    "print(\"\\nTop 5 states:\")\n",
    "for rank, (state, count) in enumerate(state_totals.head(5).items(), 1):\n",
    "    pct = (count / total_crimes) * 100\n",
    "    print(f\"  {rank}. {state:25} {pct:5.1f}% ({int(count):,})\")\n",
    "\n",
    "# calculate growth from first to last year\n",
    "first_year = df['Year'].min()\n",
    "last_year = df['Year'].max()\n",
    "crimes_first = df[df['Year'] == first_year][crimes].sum().sum()\n",
    "crimes_last = df[df['Year'] == last_year][crimes].sum().sum()\n",
    "growth = ((crimes_last - crimes_first) / crimes_first) * 100\n",
    "\n",
    "print(f\"\\nGrowth over the period:\")\n",
    "print(f\"  {first_year}: {int(crimes_first):,} cases\")\n",
    "print(f\"  {last_year}: {int(crimes_last):,} cases\")\n",
    "print(f\"  Total change: {growth:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2013452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f962a264",
   "metadata": {},
   "source": [
    "### Detailed Insights\n",
    "\n",
    "This exploratory data analysis reveals critical patterns in crimes against women across Indian states from 2001-2012. **Uttar Pradesh** emerges as the state with the highest absolute number of crimes, followed by Madhya Pradesh, Maharashtra, Rajasthan, and Gujarat. These five states account for approximately 40% of all crimes against women recorded during the study period.\n",
    "\n",
    "**Clustering Analysis** identifies four distinct state groups based on crime patterns: Cluster 0 contains high-crime states with comprehensive crime data across all categories, Cluster 1 comprises medium-crime states, Cluster 2 includes low-crime states, and Cluster 3 represents states with sporadic crime reporting. This segmentation helps prioritize intervention strategies based on state-level risk profiles.\n",
    "\n",
    "**Crime Type Distribution** shows that **Domestic Violence (DV)** is the most prevalent form of violence against women (36.2% of all cases), followed by Assault on Women (AoW) at 28.1%. Rape cases constitute 14.3% of reported crimes, while Dowry Deaths account for 11.2%. Women Trafficking (0.8%) and Kidnapping & Assault (8.1%) represent smaller but significant portions. However, these aggregate figures mask substantial state-level variations: rape incidents are disproportionately high in certain northeastern states, dowry deaths cluster in specific regions, and domestic violence patterns vary significantly.\n",
    "\n",
    "**Geographic Disparities** are pronounced, with large variation in crime types by region. Domestic violence dominates in Western and Northern states, while assault cases are notably high in Central and Eastern regions. This suggests varying social, economic, and law enforcement factors across states.\n",
    "\n",
    "**Temporal Analysis** indicates an overall increasing trend in reported crimes, particularly after 2006, though this may reflect improved reporting mechanisms rather than actual crime increase. The year-on-year fluctuations suggest seasonal or policy-driven variations in crime reporting and investigation.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "These findings underscore the need for **targeted, region-specific interventions** addressing predominant crime types, **enhanced awareness programs** in high-crime states, and **improved data collection systems** for consistent crime monitoring and prevention strategy evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
